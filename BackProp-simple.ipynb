{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do backprop in the vectorized mode (i.e., for a mini-batch).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the package and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not use GPU here. If you want to try it, you need to install pytorch with CUDA first. Please refer to [this link](https://pytorch.org/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A linear example\n",
    "\n",
    "The loss and the network are: \n",
    "\n",
    "$$ l  = agg(Y, T) \\quad\\text{, where } agg(A, B) = \\sum_{i, j} (A_{ij} - B_{ij})^2 $$\n",
    "$$ Y  = Z$$\n",
    "$$ Z  = X \\, W $$\n",
    "\n",
    "We describe the shapes of the variables below: \n",
    "* $l$: (scalar)\n",
    "* $Y$: (matrix) $n \\times C$; $C$ is the number of output classes\n",
    "* $Z$: (matrix) $n \\times C$\n",
    "* $X$: (matrix) $n \\times d$\n",
    "* $W$: (matrix) $d \\times C$\n",
    "\n",
    "You should double check if the shapes all fit together. \n",
    "\n",
    "\n",
    "![Network 1](./nn1.png \"Network 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a tiny instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, d, C = 4, 3, 2\n",
    "DOMAIN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  1.],\n",
      "        [ 1.,  2.,  0.],\n",
      "        [ 0.,  1.,  2.],\n",
      "        [ 1.,  0.,  1.]])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 1.,  2.],\n",
      "        [ 2.,  1.]]) \n",
      "\n",
      "tensor([[ 5.,  7.],\n",
      "        [ 3.,  6.],\n",
      "        [ 5.,  4.],\n",
      "        [ 3.,  3.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 6.,  4.],\n",
      "        [ 6.,  5.],\n",
      "        [ 0.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) # makes debugging and understanding a bit easier. can be commented out later. \n",
    "\n",
    "T = torch.randint(DOMAIN, (n, C), device=device, requires_grad=False) # no grad on ground truth (random ints in [0, 10])\n",
    "X = torch.randint(int(DOMAIN/2.9), (n, d), device=device, requires_grad=False) # no grad on data\n",
    "W = torch.randint(int(DOMAIN/2.9), (d, C), device=device, requires_grad=True)\n",
    "print(X)\n",
    "print(W, '\\n')\n",
    "print(X.mm(W))\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we set `requires_grad=True` for a Tensor, then any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation. If `x` is a Tensor with `requires_grad=True`, then after backpropagation `x.grad` will be another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2500)\n",
      "tensor(17.)\n"
     ]
    }
   ],
   "source": [
    "# our network (and its output Y for input X)\n",
    "Z = X.mm(W)\n",
    "Y = Z\n",
    "\n",
    "\n",
    "#loss1 = torch.nn.MSELoss(size_average=False)(Y, T) # non-averaged version\n",
    "loss1 = torch.nn.MSELoss()(Y, T) # averaged version\n",
    "loss2 = (Y-T).pow(2).sum()/2.0 # just sum all\n",
    "\n",
    "# the two loss values should differ by a factor of n. \n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  1.],\n",
      "        [-1.,  3.],\n",
      "        [ 4., -3.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loss2.backward()\n",
    "#print(Z.grad)\n",
    "print(W.grad)\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare with the manually computed gradient: \n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial W} =  \\frac{\\partial l}{\\partial Y}  \\frac{\\partial Y}{\\partial W} = X^{\\top} (Y-T) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  1.],\n",
       "        [-1.,  3.],\n",
       "        [ 4., -3.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(X).mm(Y-T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A non-linear example\n",
    "\n",
    "The loss and the network are: \n",
    "\n",
    "$$ l  = agg(Y, T) \\quad\\text{, where } agg(A, B) = \\sum_{i, j} (A_{ij} - B_{ij})^2 $$\n",
    "$$ Y  = \\sigma(Z) $$\n",
    "$$ Z  = X \\, W $$\n",
    "\n",
    "$\\sigma(\\cdot)$ is the usual sigmoid function. \n",
    "\n",
    "We describe the shapes of the variables below: \n",
    "* $l$: (scalar)\n",
    "* $Y$: (matrix) $n \\times C$; $C$ is the number of output classes\n",
    "* $Z$: (matrix) $n \\times C$\n",
    "* $X$: (matrix) $n \\times d$\n",
    "* $W$: (matrix) $d \\times C$\n",
    "\n",
    "You should double check if the shapes all fit together. If you do this, you will see that \n",
    "* The non-linear transformation, $\\sigma$, is applied element-wise to the input matrix.\n",
    "\n",
    "![Network 2](./nn2.png \"Network 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.3651)\n",
      "tensor(61.4605)\n"
     ]
    }
   ],
   "source": [
    "#reset the gradients\n",
    "W.grad.data.zero_()\n",
    "\n",
    "sigma = torch.sigmoid\n",
    "\n",
    "# our network (and its output Y for input X)\n",
    "Z = X.mm(W)\n",
    "Y = sigma(Z)\n",
    "\n",
    "loss1 = torch.nn.MSELoss()(Y, T) # averaged version\n",
    "loss2 = (Y-T).pow(2).sum()/2.0 # just sum all\n",
    "\n",
    "# the two loss values should differ by a factor of n. \n",
    "print(loss1)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1917, -0.1505],\n",
      "        [-0.5027, -0.0967],\n",
      "        [-0.0302, -0.2851]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loss2.backward()\n",
    "#print(Z.grad)\n",
    "print(W.grad)\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare with the manually computed gradient: \n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial W} =  \\frac{\\partial l}{\\partial Y} \\frac{\\partial Y}{\\partial Z} \\frac{\\partial Y}{\\partial W} = X^{\\top} (Y \\circ (1-Y) \\circ (Y-T)) $$\n",
    "\n",
    "where $\\circ$ is the Hadamard product (https://en.wikipedia.org/wiki/Hadamard_product_(matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1917, -0.1505],\n",
       "        [-0.5027, -0.0967],\n",
       "        [-0.0302, -0.2851]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_w = torch.t(X).mm(Y * (1-Y) * (Y-T))\n",
    "gradient_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: Gradient checking\n",
    "\n",
    "You can read more about [Gradient Checking](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization). You need to implement it and check against the `gradient_w` computed above.\n",
    "\n",
    "If you use `EPSILON = 1e-4`, you should get sth close to the following, where the three columns are: \n",
    "* gradient_w[i]\n",
    "* numerically computed gradient_w[i]\n",
    "* difference of the two\n",
    "\n",
    "````\n",
    "-0.19160029292106628\t-0.19073486328125\t-0.0008654296398162842\n",
    "-0.15006516873836517\t-0.171661376953125\t0.021596208214759827\n",
    "-0.5024182796478271\t-0.514984130859375\t0.012565851211547852\n",
    "-0.09660282731056213\t-0.095367431640625\t-0.0012353956699371338\n",
    "-0.03004339337348938\t-0.03814697265625\t0.00810357928276062\n",
    "-0.28447985649108887\t-0.286102294921875\t0.0016224384307861328\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-05 *\n",
      "       [[10.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "tensor(1.00000e-05 *\n",
      "       [[ 0.0000, 10.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "tensor(1.00000e-05 *\n",
      "       [[ 0.0000,  0.0000],\n",
      "        [10.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "tensor(1.00000e-05 *\n",
      "       [[ 0.0000,  0.0000],\n",
      "        [ 0.0000, 10.0000],\n",
      "        [ 0.0000,  0.0000]])\n",
      "tensor(1.00000e-05 *\n",
      "       [[ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [10.0000,  0.0000]])\n",
      "tensor(1.00000e-05 *\n",
      "       [[ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000, 10.0000]])\n",
      "-0.19168421626091003\t-0.19073486328125\t-0.0009493529796600342\n",
      "-0.1505398154258728\t-0.171661376953125\t0.021121561527252197\n",
      "-0.5027213096618652\t-0.514984130859375\t0.012262821197509766\n",
      "-0.09670328348875046\t-0.095367431640625\t-0.0013358518481254578\n",
      "-0.03022773563861847\t-0.03814697265625\t0.00791923701763153\n",
      "-0.28507155179977417\t-0.286102294921875\t0.00103074312210083\n"
     ]
    }
   ],
   "source": [
    "DEBUG = 1\n",
    "epsilon = 1e-4\n",
    "\n",
    "\n",
    "# input W, output the loss2\n",
    "def f(WW): \n",
    "    Z = X.mm(WW)\n",
    "    Y = sigma(Z)\n",
    "    return (Y-T).pow(2).sum()/2.0 # just sum all\n",
    "    \n",
    "\n",
    "# gradient wrt W\n",
    "numerical_gradient = []\n",
    "with torch.no_grad(): # prevent accumulating the gradient. \n",
    "    \n",
    "    for i in range(d * C): \n",
    "        # create perturbation in one axis\n",
    "        perturbation = torch.zeros(d, C) # same size as W\n",
    "        perturbation.view(d * C)[i] = 1.0\n",
    "        perturbation = perturbation * epsilon \n",
    "        # \n",
    "        if DEBUG: \n",
    "            print(perturbation)\n",
    "\n",
    "        # two loss2 values\n",
    "        loss2_left  = f(W - perturbation)\n",
    "        loss2_right = f(W + perturbation)\n",
    "\n",
    "        # numeric gradient in one axis:\n",
    "        val = (loss2_right.item() - loss2_left.item() ) / (2 * epsilon)\n",
    "        numerical_gradient.append(val)\n",
    "    # end for\n",
    "    \n",
    "    # print out\n",
    "    grad_w = torch.t(X).mm(Y * (1-Y) * (Y-T)).reshape(d*C) # redo it to make the code self-contained.\n",
    "    grad_w_numpy = grad_w.data.numpy()\n",
    "    for i in range(d * C):\n",
    "        print('{:>10}\\t{:>10}\\t{:>10}'.format(grad_w_numpy[i], \n",
    "                                              numerical_gradient[i], \n",
    "                                              grad_w_numpy[i]-numerical_gradient[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
